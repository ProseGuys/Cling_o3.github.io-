<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1"><title>PDF 格式化提取调研</title>
<meta charset=utf-8><meta name=description content="Ladder@在整理PDF 格式化内容提取方案时，发现当前网上所提及的库要么落后于时代版本要么就是各种文字描述语焉不详。因此根据调研结果重新整理"><meta name=author content="人间客"><link rel=canonical href=https://proseguys.github.io/blog/pdf_extractor_report/><meta name=google-site-verification content="xxx"><link rel=alternate type=application/rss+xml href=https://proseguys.github.io//index.xml title=人间客><script async src="https://www.googletagmanager.com/gtag/js?id=G-xxx"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-xxx")}</script><script async defer data-website-id=xxx src=https://umami-ochre-nu.vercel.app/hugo-ladder.js></script><meta property="og:url" content="https://proseguys.github.io/blog/pdf_extractor_report/"><meta property="og:site_name" content="人间客"><meta property="og:title" content="PDF 格式化提取调研"><meta property="og:description" content="在整理PDF 格式化内容提取方案时，发现当前网上所提及的库要么落后于时代版本要么就是各种文字描述语焉不详。因此根据调研结果重新整理"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-15T15:54:10+08:00"><meta property="article:modified_time" content="2024-08-15T15:54:10+08:00"><meta property="article:tag" content="PDF"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="PDF 格式化提取调研"><meta name=twitter:description content="在整理PDF 格式化内容提取方案时，发现当前网上所提及的库要么落后于时代版本要么就是各种文字描述语焉不详。因此根据调研结果重新整理"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://proseguys.github.io/blog/"},{"@type":"ListItem","position":2,"name":"PDF 格式化提取调研","item":"https://proseguys.github.io/blog/pdf_extractor_report/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PDF 格式化提取调研","name":"PDF 格式化提取调研","description":"在整理PDF 格式化内容提取方案时，发现当前网上所提及的库要么落后于时代版本要么就是各种文字描述语焉不详。因此根据调研结果重新整理\n","keywords":["PDF","AI"],"articleBody":"在整理PDF 格式化内容提取方案时，发现当前网上所提及的库要么落后于时代版本要么就是各种文字描述语焉不详。因此根据调研结果重新整理\n需求： 提取PDF文件内容，如果PDF文件中数据之间包含段落关系，需保留其段落关系。\n困难点： 由于PDF文件可以有多种来源生成，如html文件转换、markdown文件转换、文本影印等。部分来源文件天然不存在段落关系或者文字信息不清晰等，因此本文调研优先考虑由html和markdown等层级化文件转换成的PDF文件内容提取。\n实验方案： 使用由markdown文件生成的PDF文件进行内容提取，对比各自提取结构和结果\n测试文件：\ntest.pdf\n调研对象 pdfplumber pdfminer.six PyMuPDF pypdfium2 marker MinerU pdfplumber **文档地址：**https://github.com/jsvine/pdfplumber\n**简介：**查看PDF文件以获取有关每个文本字符、矩形和线条的详细信息。并具有表格提取和可视化调试能力，基于pdfminer.six\n**测试结果：**根据PDF的单页作为内容操作的基本单位，只支持获取纯文本信息，根据段落结构获取方式不成熟，以下为测试结果\n直接文字提取 import pdfplumber def pdf_to_markdown(pdf_path: str): # 使用 pdfplumber 打开 PDF 文件 with pdfplumber.open(pdf_path) as pdf: for page in pdf.pages: # 提取每页的 HTML 内容, context = page.extract_text() print(context) pdf_to_markdown(\"pdf_test/test.pdf\") 丢失文件段落结构，标题正文无法区分。\n段落结构获取(实验阶段) import pdfplumber def pdf_to_markdown(pdf_path: str): # 使用 pdfplumber 打开 PDF 文件 with pdfplumber.open(pdf_path) as pdf: for page in pdf.pages: # 提取每页的 HTML 内容 context = page.extract_text(layout=True) print(context) pdf_to_markdown(\"pdf_test/test.pdf\") 丢失标题结构，且增加对应换行符，增加后续解析难度\npdfminer.six 文档地址： https://pdfminersix.readthedocs.io/en/latest/\n简介： Pdfminer.6 是社区维护的原始 PDFMiner 的分支。它是一个从PDF文档中提取信息的工具。它专注于获取和分析文本数据。 Pdfminer.6 直接从 PDF 源代码中提取页面文本。它还可用于获取文本的确切位置、字体或颜色。\n测试结果： 通过extract_text 方法直接获取整页PDF的文本信息，丢失文本层级结构\nfrom pdfminer.high_level import extract_text def pdf_to_markdown(pdf_path: str): text = extract_text(pdf_path) with open(\"./page.txt\", \"w\") as f: f.write(text) print(text) pdf_to_markdown(\"pdf_test/test.pdf\") 丢失部分PDF文档结构，丢失标题层级\nPyMuPDF 文档地址： https://pymupdf.readthedocs.io/en/latest/tutorial.html\n简介： 高性能python库，用于PDF的数据提取、分析、转换和操作。\nPyMuPDF Pro还支持 office文件的支持，包括：\nDOC/DOCX 文档/文档CX PPT/PPTX XLS/XLSX HWP/HWPX 模块基础单元为Document实例，用户可以通过PyMuPDF打开文件，获取Document实例。Document包含许多属性和功能。其中包括元信息（如“作者”或“主题”）、总页数、大纲和加密信息。\n页面处理是PyMuPDF功能的核心，用户可以通过Document作为Page的迭代器，通过循环获取不同的Page。\n用户可以通过不同形式和详细程度提取页面的所有文本、图像和其他信息。\ntext = page.get_text(opt) 使用一下option选项获取不同的格式：\n“text” ：（默认）带有换行符的纯文本。没有格式、没有文本位置详细信息、没有图像。 “blocks” ：生成文本块（=段落）列表。 “words” ：生成单词列表（不包含空格的字符串）。 “html” ：创建页面的完整视觉版本，包括任何图像。这可以通过您的互联网浏览器显示。 “dict” / “json” ：与 HTML 相同的信息级别，但以 Python 字典或 resp 形式提供。 JSON 字符串。有关其结构的详细信息，请参阅TextPage.extractDICT() 。 “rawdict” / “rawjson” ： “dict” / **“json”**的超集。它还提供诸如 XML 之类的字符详细信息。有关其结构的详细信息，请参阅TextPage.extractRAWDICT() 。 “xhtml” ：文本信息级别为TEXT版本，但包含图像。也可以通过互联网浏览器显示。 “xml” ：不包含图像，但包含完整的位置和字体信息，具体到每个文本字符。使用 XML 模块进行解释。 PyMuPDF支持使用story类通过html源生成PDF，并用于解析。然而并不提供直接通过Stroy实例转换成Document实例的方法。story类相关文档：https://pymupdf.readthedocs.io/en/latest/recipes-stories.html#stories\n测试结果： 根据PDF的单页作为内容操作的基本单位，保留文件结构，并提供文件大纲进行标题区分，可以通过大纲和文本内容区分相关结构，以下为测试结果\ndoc.get_toc() # 获取Document的大纲信息 import pymupdf content = \"\" doc = pymupdf.open(\"./pdf_test/test.pdf\") for page in doc: text = page.get_text() print(text) content += text with open(\"./pdf_test/pymupdf.txt\", \"w\") as f: f.write(content) pypdfium2 文档地址： https://github.com/pypdfium2-team/pypdfium2\n简介： pypdfium2是与PDFium绑定的ABI 级Python 3，PDFium 是一个功能强大且经过自由许可的库，用于 PDF 渲染、检查、操作和创建。\n测试结果： 与PyMuPDF类似，通过Document以及Page来实现页面文本信息的获取，也支持获取文件大纲用于判断文档标题，文本内容在获取时会丢失相关层级结构\nimport pypdfium2 as pdfium pdf = pdfium.PdfDocument(\"./pdf_test/test.pdf\") for item in pdf.get_toc(): state = \"*\" if item.n_kids == 0 else \"-\" if item.is_closed else \"+\" target = \"?\" if item.page_index is None else item.page_index + 1 print( \" \" * item.level + \"[%s] %s -\u003e %s # %s %s\" % ( state, item.title, target, item.view_mode, item.view_pos, ) ) print(list(pdf.get_toc())) 可以展示PDF文件的层级结构\nimport pypdfium2 as pdfium pdf = pdfium.PdfDocument(\"./pdf_test/test.pdf\") content = \"\" for page in pdf: textpage = page.get_textpage() text_all = textpage.get_text_range() print(text_all) content += text_all with open(\"pdf_test/pypdfium2.txt\", \"w\") as f: f.write(content) 文本提取不如PyMuPDF结构，不过依旧可以通过大纲和标题获取文件内容的层级结构。\nMinerU 文档地址：https://github.com/VikParuchuri/marker\n简介： 是一款一站式、开源、高质量的数据提取工具，主要提供以下主要功能：\nMagic-PDF：是一款旨在将PDF文档转换为Markdown格式的工具，能够处理本地存储或支持S3协议的对象存储上的文件。 支持多种前端模型输入 删除页眉、页脚、脚注和页码 人类可读的布局格式 保留原始文档的结构和格式，包括标题、段落、列表等 Markdown 图像和表格的提取和显示 自动检测并转换PDF乱码 将方程转换为 LaTeX 格式 与CPU和GPU环境的兼容性 适用于 Windows、Linux 和 macOS 平台 基于PDF-Extract-Kit作为PDF内容提取的工具 Magic-Doc：是一款旨在将网页或多格式电子书转换为Markdown格式的工具。 网页提取：文本、图像、表格、公式信息的跨模态精准解析。 电子书文档提取：支持epub、mobi等多种文档格式，文本、图片全面适配。 语言类型识别：准确识别176种语言。 本文主要记录Magic-PDF对于PDF解析效果，Magic-PDF基于PDF-Extract-Kit进行PDF内容提取，因此需要下载PaddleOcr和其自定义训练的模型。模型大小约5G左右。\n测试结果： 由于Magic-PDF模块安装复杂，因此使用命令行形式进行测试\nmagic-pdf pdf-command --pdf \"pdf_path/test.pdf\" --inside_model true 成功执行后，会在设定好的temp-output-dir目录下生成magic-pdf文件夹。每单次执行会在此目录下生成一个文件夹内部包含解析文件\nlayout.pdf和spans.pdf代表源PDF文件的内部元素的布局和宽度\n后缀为md的文件代表其生成的Markdown文件：\n后缀为json的文件代表各个元素的各类信息。\n通过与源文件对比，可发现存在部分元素提取异常，元素结构提取错误。标题格式误识别率较高。如下图”DNS解析流程“应该为正文内容，这里却将它识别成了标题。\nmarker 文档地址： https://github.com/VikParuchuri/marker\n简介： 快速准确地将 PDF 转换为 Markdown。\n支持多种文档（针对书籍和科学论文进行了优化） 支持所有语言 清除页眉/页脚/其他杂物 格式化表格和代码块 提取并保存图像以及相应的 markdown 将大多数方程转换为 latex 可在 GPU、CPU 或 MPS 上运行 测试结果：通过 convert_single_pdf方法直接获取整页PDF的文本信息，可以提取部分文档层级结构，可以提取代码、表格、公式，页眉页脚未能正确去除。\nfrom marker.convert import convert_single_pdf from marker.models import load_all_models full_text, images, out_meta = convert_single_pdf( \"./test2.pdf\", load_all_models(), langs=[\"Chinese\", \"English\"], batch_multiplier=2, ) print(full_text) print(images) print(out_meta) 部分层级结构保留 如下，部分文本应为正文，但却被识别成标题结构。\n实际测试中，页眉页脚未能正确去除\n页眉页脚未能去除 需要从 huggingface 导入 6 个模型，显存要求较高，测试时出现爆显存现象\nmodel_lst = [texify, layout, order, edit, detection, ocr]\n小结： 在以上模块中，如果只需要获取PDF文本信息的前提下，PyMuPDF模块的效果最好，文本提取准确，提供大纲方法获取文本标题，从而分析得出文本结构。\n若需要PDF转成Markdown格式，可以使用MinerU模块，不过当前存在模块内部依赖混乱，外部调用API不完善等问题\n","wordCount":"410","inLanguage":"en","datePublished":"2024-08-15T15:54:10+08:00","dateModified":"2024-08-15T15:54:10+08:00","author":{"@type":"Person","name":"人间客"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://proseguys.github.io/blog/pdf_extractor_report/"},"publisher":{"@type":"Organization","name":"人间客","logo":{"@type":"ImageObject","url":"https://proseguys.github.io/favicon.ico"}}}</script><link rel=icon href=/images/avatar.png sizes=16x16><link rel=apple-touch-icon href=/images/avatar.png><link rel=manifest href=/images/avatar.png><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.7.0/style.css><link rel=stylesheet href=/css/main.min.ec28f09e946fc0df77c187fcd0d0ebde58fca6de8efb8e1620f3d45c32d4da88.css integrity="sha256-7CjwnpRvwN93wYf80NDr3lj8pt6O+44WIPPUXDLU2og=" crossorigin=anonymous media=screen><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js></script><script>hljs.highlightAll()</script><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand href=/>HOME
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link href=/blog>文章</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/tags>分类</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/archives>历史文章</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/guestbook>留言板</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=https://umami-ochre-nu.vercel.app/share/R1lHz7QY/hugo-ladder-exampleSite>网站统计</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link href=https://github.com/ProseGuys><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span>
<span class=toggle-light><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>PDF 格式化提取调研</h1></header><p><small>August 15, 2024&nbsp;· 410 words&nbsp;· 2 min</small>
<small>·
<a href=https://proseguys.github.io/tags/pdf/>PDF</a>
<a href=https://proseguys.github.io/tags/ai/>AI</a></small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#需求>需求：</a></li><li><a href=#困难点>困难点：</a></li><li><a href=#实验方案>实验方案：</a></li><li><a href=#调研对象>调研对象</a><ul><li><a href=#pdfplumber>pdfplumber</a></li><li><a href=#pdfminersix><strong>pdfminer.six</strong></a></li><li><a href=#pymupdf>PyMuPDF</a></li><li><a href=#pypdfium2>pypdfium2</a></li><li><a href=#mineru>MinerU</a></li><li><a href=#marker>marker</a></li></ul></li><li><a href=#小结>小结：</a></li></ul></nav></div><section class=blog-content><p>在整理PDF 格式化内容提取方案时，发现当前网上所提及的库要么落后于时代版本要么就是各种文字描述语焉不详。因此根据调研结果重新整理</p><h2 id=需求>需求：</h2><p>提取PDF文件内容，如果PDF文件中数据之间包含段落关系，需保留其段落关系。</p><h2 id=困难点>困难点：</h2><p>由于PDF文件可以有多种来源生成，如html文件转换、markdown文件转换、文本影印等。部分来源文件天然不存在段落关系或者文字信息不清晰等，因此本文调研优先考虑由html和markdown等层级化文件转换成的PDF文件内容提取。</p><h2 id=实验方案>实验方案：</h2><p>使用由markdown文件生成的PDF文件进行内容提取，对比各自提取结构和结果</p><p>测试文件：</p><p><a href=/images/pdf_extractor_report/test.pdf>test.pdf</a></p><h2 id=调研对象>调研对象</h2><ul><li>pdfplumber</li><li>pdfminer.six</li><li>PyMuPDF</li><li>pypdfium2</li><li>marker</li><li>MinerU</li></ul><h3 id=pdfplumber>pdfplumber</h3><p>**文档地址：**<a href=https://github.com/jsvine/pdfplumber>https://github.com/jsvine/pdfplumber</a></p><p>**简介：**查看PDF文件以获取有关每个文本字符、矩形和线条的详细信息。并具有表格提取和可视化调试能力，基于pdfminer.six</p><p>**测试结果：**根据PDF的单页作为内容操作的基本单位，只支持获取纯文本信息，根据段落结构获取方式不成熟，以下为测试结果</p><ul><li>直接文字提取</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pdfplumber
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pdf_to_markdown</span>(pdf_path: str):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 使用 pdfplumber 打开 PDF 文件</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> pdfplumber<span style=color:#f92672>.</span>open(pdf_path) <span style=color:#66d9ef>as</span> pdf:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> page <span style=color:#f92672>in</span> pdf<span style=color:#f92672>.</span>pages:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 提取每页的 HTML 内容, </span>
</span></span><span style=display:flex><span>            context <span style=color:#f92672>=</span> page<span style=color:#f92672>.</span>extract_text()
</span></span><span style=display:flex><span>            print(context)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pdf_to_markdown(<span style=color:#e6db74>&#34;pdf_test/test.pdf&#34;</span>)
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/Untitled.png alt=Untitled></p><p>丢失文件段落结构，标题正文无法区分。</p><ul><li>段落结构获取(实验阶段)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pdfplumber
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pdf_to_markdown</span>(pdf_path: str):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 使用 pdfplumber 打开 PDF 文件</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> pdfplumber<span style=color:#f92672>.</span>open(pdf_path) <span style=color:#66d9ef>as</span> pdf:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> page <span style=color:#f92672>in</span> pdf<span style=color:#f92672>.</span>pages:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 提取每页的 HTML 内容</span>
</span></span><span style=display:flex><span>            context <span style=color:#f92672>=</span> page<span style=color:#f92672>.</span>extract_text(layout<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>            print(context)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pdf_to_markdown(<span style=color:#e6db74>&#34;pdf_test/test.pdf&#34;</span>)
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/Untitled%201.png alt=Untitled></p><p>丢失标题结构，且增加对应换行符，增加后续解析难度</p><h3 id=pdfminersix><strong>pdfminer.six</strong></h3><p><strong>文档地址：</strong> <a href=https://pdfminersix.readthedocs.io/en/latest/>https://pdfminersix.readthedocs.io/en/latest/</a></p><p><strong>简介：</strong> Pdfminer.6 是社区维护的原始 PDFMiner 的分支。它是一个从PDF文档中提取信息的工具。它专注于获取和分析文本数据。 Pdfminer.6 直接从 PDF 源代码中提取页面文本。它还可用于获取文本的确切位置、字体或颜色。</p><p><strong>测试结果：</strong> 通过<code>extract_text</code> 方法直接获取整页PDF的文本信息，丢失文本层级结构</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pdfminer.high_level <span style=color:#f92672>import</span> extract_text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pdf_to_markdown</span>(pdf_path: str):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> extract_text(pdf_path)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;./page.txt&#34;</span>, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        f<span style=color:#f92672>.</span>write(text)
</span></span><span style=display:flex><span>    print(text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pdf_to_markdown(<span style=color:#e6db74>&#34;pdf_test/test.pdf&#34;</span>)
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/Untitled%202.png alt=Untitled></p><p>丢失部分PDF文档结构，丢失标题层级</p><h3 id=pymupdf>PyMuPDF</h3><p><strong>文档地址：</strong> <a href=https://pymupdf.readthedocs.io/en/latest/tutorial.html>https://pymupdf.readthedocs.io/en/latest/tutorial.html</a></p><p><strong>简介：</strong> 高性能python库，用于PDF的数据提取、分析、转换和操作。</p><p>PyMuPDF Pro还支持 office文件的支持，包括：</p><ul><li>DOC/DOCX 文档/文档CX</li><li>PPT/PPTX</li><li>XLS/XLSX</li><li>HWP/HWPX</li></ul><p>模块基础单元为Document实例，用户可以通过PyMuPDF打开文件，获取Document实例。Document包含许多属性和功能。其中包括元信息（如“作者”或“主题”）、总页数、大纲和加密信息。</p><p>页面处理是PyMuPDF功能的核心，用户可以通过Document作为Page的迭代器，通过循环获取不同的Page。</p><p>用户可以通过不同形式和详细程度提取页面的所有文本、图像和其他信息。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>text <span style=color:#f92672>=</span> page<span style=color:#f92672>.</span>get_text(opt)
</span></span></code></pre></div><p>使用一下option选项获取不同的格式：</p><ul><li><strong>“text”</strong> ：（默认）带有换行符的纯文本。没有格式、没有文本位置详细信息、没有图像。</li><li><strong>“blocks”</strong> ：生成文本块（=段落）列表。</li><li><strong>“words”</strong> ：生成单词列表（不包含空格的字符串）。</li><li><strong>“html”</strong> ：创建页面的完整视觉版本，包括任何图像。这可以通过您的互联网浏览器显示。</li><li><strong>“dict”</strong> / <strong>“json”</strong> ：与 HTML 相同的信息级别，但以 Python 字典或 resp 形式提供。 JSON 字符串。有关其结构的详细信息，请参阅<a href=https://pymupdf.readthedocs.io/en/latest/textpage.html#TextPage.extractDICT><code>TextPage.extractDICT()</code></a> 。</li><li><strong>“rawdict”</strong> / <strong>“rawjson”</strong> ： <strong>“dict”</strong> / **“json”**的超集。它还提供诸如 XML 之类的字符详细信息。有关其结构的详细信息，请参阅<a href=https://pymupdf.readthedocs.io/en/latest/textpage.html#TextPage.extractRAWDICT><code>TextPage.extractRAWDICT()</code></a> 。</li><li><strong>“xhtml”</strong> ：文本信息级别为TEXT版本，但包含图像。也可以通过互联网浏览器显示。</li><li><strong>“xml”</strong> ：不包含图像，但包含完整的位置和字体信息，具体到每个文本字符。使用 XML 模块进行解释。</li></ul><p>PyMuPDF支持使用story类通过html源生成PDF，并用于解析。然而并不提供直接通过Stroy实例转换成Document实例的方法。story类相关文档：https://pymupdf.readthedocs.io/en/latest/recipes-stories.html#stories</p><p><strong>测试结果：</strong> 根据PDF的单页作为内容操作的基本单位，保留文件结构，并提供文件大纲进行标题区分，可以通过大纲和文本内容区分相关结构，以下为测试结果</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>doc<span style=color:#f92672>.</span>get_toc() <span style=color:#75715e># 获取Document的大纲信息</span>
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/Untitled%203.png alt=Untitled></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pymupdf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>content <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>doc <span style=color:#f92672>=</span> pymupdf<span style=color:#f92672>.</span>open(<span style=color:#e6db74>&#34;./pdf_test/test.pdf&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> page <span style=color:#f92672>in</span> doc:
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> page<span style=color:#f92672>.</span>get_text()
</span></span><span style=display:flex><span>    print(text)
</span></span><span style=display:flex><span>    content <span style=color:#f92672>+=</span> text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;./pdf_test/pymupdf.txt&#34;</span>, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    f<span style=color:#f92672>.</span>write(content)
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/Untitled%204.png alt=Untitled></p><p><img src=/images/pdf_extractor_report/Untitled%205.png alt=Untitled></p><h3 id=pypdfium2>pypdfium2</h3><p><strong>文档地址：</strong> <a href=https://github.com/pypdfium2-team/pypdfium2>https://github.com/pypdfium2-team/pypdfium2</a></p><p><strong>简介：</strong> pypdfium2是与PDFium绑定的ABI 级Python 3，PDFium 是一个功能强大且经过自由许可的库，用于 PDF 渲染、检查、操作和创建。</p><p><strong>测试结果：</strong> 与PyMuPDF类似，通过Document以及Page来实现页面文本信息的获取，也支持获取文件大纲用于判断文档标题，文本内容在获取时会丢失相关层级结构</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pypdfium2 <span style=color:#66d9ef>as</span> pdfium
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pdf <span style=color:#f92672>=</span> pdfium<span style=color:#f92672>.</span>PdfDocument(<span style=color:#e6db74>&#34;./pdf_test/test.pdf&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> pdf<span style=color:#f92672>.</span>get_toc():
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;*&#34;</span> <span style=color:#66d9ef>if</span> item<span style=color:#f92672>.</span>n_kids <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;-&#34;</span> <span style=color:#66d9ef>if</span> item<span style=color:#f92672>.</span>is_closed <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;+&#34;</span>
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;?&#34;</span> <span style=color:#66d9ef>if</span> item<span style=color:#f92672>.</span>page_index <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span> <span style=color:#66d9ef>else</span> item<span style=color:#f92672>.</span>page_index <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    print(
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;    &#34;</span> <span style=color:#f92672>*</span> item<span style=color:#f92672>.</span>level
</span></span><span style=display:flex><span>        <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;[</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>] </span><span style=color:#e6db74>%s</span><span style=color:#e6db74> -&gt; </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>  # </span><span style=color:#e6db74>%s</span><span style=color:#e6db74> </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>%</span> (
</span></span><span style=display:flex><span>            state,
</span></span><span style=display:flex><span>            item<span style=color:#f92672>.</span>title,
</span></span><span style=display:flex><span>            target,
</span></span><span style=display:flex><span>            item<span style=color:#f92672>.</span>view_mode,
</span></span><span style=display:flex><span>            item<span style=color:#f92672>.</span>view_pos,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>print(list(pdf<span style=color:#f92672>.</span>get_toc()))
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/Untitled%206.png alt=Untitled></p><p>可以展示PDF文件的层级结构</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pypdfium2 <span style=color:#66d9ef>as</span> pdfium
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pdf <span style=color:#f92672>=</span> pdfium<span style=color:#f92672>.</span>PdfDocument(<span style=color:#e6db74>&#34;./pdf_test/test.pdf&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>content <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> page <span style=color:#f92672>in</span> pdf:
</span></span><span style=display:flex><span>    textpage <span style=color:#f92672>=</span> page<span style=color:#f92672>.</span>get_textpage()
</span></span><span style=display:flex><span>    text_all <span style=color:#f92672>=</span> textpage<span style=color:#f92672>.</span>get_text_range()
</span></span><span style=display:flex><span>    print(text_all)
</span></span><span style=display:flex><span>    content <span style=color:#f92672>+=</span> text_all
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;pdf_test/pypdfium2.txt&#34;</span>, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    f<span style=color:#f92672>.</span>write(content)
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/Untitled%207.png alt=Untitled></p><p><img src=/images/pdf_extractor_report/Untitled%208.png alt=Untitled></p><p>文本提取不如PyMuPDF结构，不过依旧可以通过大纲和标题获取文件内容的层级结构。</p><h3 id=mineru>MinerU</h3><p>文档地址：https://github.com/VikParuchuri/marker</p><p><strong>简介：</strong> 是一款一站式、开源、高质量的数据提取工具，主要提供以下主要功能：</p><ul><li>Magic-PDF：是一款旨在将PDF文档转换为Markdown格式的工具，能够处理本地存储或支持S3协议的对象存储上的文件。<ul><li>支持多种前端模型输入</li><li>删除页眉、页脚、脚注和页码</li><li>人类可读的布局格式</li><li>保留原始文档的结构和格式，包括标题、段落、列表等</li><li>Markdown 图像和表格的提取和显示</li><li>自动检测并转换PDF乱码</li><li>将方程转换为 LaTeX 格式</li><li>与CPU和GPU环境的兼容性</li><li>适用于 Windows、Linux 和 macOS 平台</li><li>基于PDF-Extract-Kit作为PDF内容提取的工具</li></ul></li><li>Magic-Doc：是一款旨在将网页或多格式电子书转换为Markdown格式的工具。<ul><li>网页提取：文本、图像、表格、公式信息的跨模态精准解析。</li><li>电子书文档提取：支持epub、mobi等多种文档格式，文本、图片全面适配。</li><li>语言类型识别：准确识别176种语言。</li></ul></li></ul><p>本文主要记录Magic-PDF对于PDF解析效果，Magic-PDF基于PDF-Extract-Kit进行PDF内容提取，因此需要下载PaddleOcr和其自定义训练的模型。模型大小约5G左右。</p><p><strong>测试结果：</strong> 由于Magic-PDF模块安装复杂，因此使用命令行形式进行测试</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>magic<span style=color:#f92672>-</span>pdf pdf<span style=color:#f92672>-</span>command <span style=color:#f92672>--</span>pdf <span style=color:#e6db74>&#34;pdf_path/test.pdf&#34;</span> <span style=color:#f92672>--</span>inside_model true
</span></span></code></pre></div><p>成功执行后，会在设定好的temp-output-dir目录下生成magic-pdf文件夹。每单次执行会在此目录下生成一个文件夹内部包含解析文件</p><p><img src=/images/pdf_extractor_report/Untitled%209.png alt=Untitled></p><p>layout.pdf和spans.pdf代表源PDF文件的内部元素的布局和宽度</p><p><img src=/images/pdf_extractor_report/Untitled%2010.png alt=Untitled></p><p><img src=/images/pdf_extractor_report/Untitled%2011.png alt=Untitled></p><p>后缀为md的文件代表其生成的Markdown文件：</p><p><img src=/images/pdf_extractor_report/Untitled%2012.png alt=Untitled></p><p>后缀为json的文件代表各个元素的各类信息。</p><p>通过与源文件对比，可发现存在部分元素提取异常，元素结构提取错误。标题格式误识别率较高。如下图”DNS解析流程“应该为正文内容，这里却将它识别成了标题。</p><p><img src=/images/pdf_extractor_report/Untitled%2013.png alt=Untitled></p><h3 id=marker>marker</h3><p><strong>文档地址：</strong> <a href=https://github.com/VikParuchuri/marker>https://github.com/VikParuchuri/marker</a></p><p><strong>简介：</strong> 快速准确地将 PDF 转换为 Markdown。</p><ul><li>支持多种文档（针对书籍和科学论文进行了优化）</li><li>支持所有语言</li><li>清除页眉/页脚/其他杂物</li><li>格式化表格和代码块</li><li>提取并保存图像以及相应的 markdown</li><li>将大多数方程转换为 latex</li><li>可在 GPU、CPU 或 MPS 上运行</li></ul><p><strong>测试结果：通过 <code>convert_single_pdf</code>方法直接获取整页PDF的文本信息，可以提取部分文档层级结构，可以提取代码、表格、公式，页眉页脚未能正确去除。</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> marker.convert <span style=color:#f92672>import</span> convert_single_pdf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> marker.models <span style=color:#f92672>import</span> load_all_models
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>full_text, images, out_meta <span style=color:#f92672>=</span> convert_single_pdf(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;./test2.pdf&#34;</span>,
</span></span><span style=display:flex><span>    load_all_models(),
</span></span><span style=display:flex><span>    langs<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;Chinese&#34;</span>, <span style=color:#e6db74>&#34;English&#34;</span>],
</span></span><span style=display:flex><span>    batch_multiplier<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(full_text)
</span></span><span style=display:flex><span>print(images)
</span></span><span style=display:flex><span>print(out_meta)
</span></span></code></pre></div><p><img src=/images/pdf_extractor_report/image.png alt=image.png></p><p><img src=/images/pdf_extractor_report/image%201.png alt=image.png></p><p><img src=/images/pdf_extractor_report/image%202.png alt=image.png></p><pre><code>                                                                      部分层级结构保留
</code></pre><p>如下，部分文本应为正文，但却被识别成标题结构。</p><p><img src=/images/pdf_extractor_report/image%203.png alt=image.png></p><p><img src=/images/pdf_extractor_report/image%204.png alt=image.png></p><p>实际测试中，页眉页脚未能正确去除</p><p><img src=/images/pdf_extractor_report/image%205.png alt=image.png></p><pre><code>                                                                        页眉页脚未能去除
</code></pre><p>需要从 huggingface 导入 6 个模型，显存要求较高，测试时出现爆显存现象</p><p><code>model_lst = [texify, layout, order, edit, detection, ocr]</code></p><p><img src=/images/pdf_extractor_report/image%206.png alt=image.png></p><h2 id=小结>小结：</h2><p>在以上模块中，如果只需要获取PDF文本信息的前提下，PyMuPDF模块的效果最好，文本提取准确，提供大纲方法获取文本标题，从而分析得出文本结构。</p><p>若需要PDF转成Markdown格式，可以使用MinerU模块，不过当前存在模块内部依赖混乱，外部调用API不完善等问题</p></section><div class=comments><script>const getTheme=window.localStorage&&window.localStorage.getItem("theme");let theme=getTheme==="dark"?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","ProseGuys/ProseGuys.github.io"),s.setAttribute("data-repo-id","R_kgDOMBAKKw"),s.setAttribute("data-category","Announcements"),s.setAttribute("data-category-id","DIC_kwDOMBAKK84CfppF"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","bottom"),s.setAttribute("data-theme",theme),s.setAttribute("data-lang","zh-CN"),s.setAttribute("data-loading","lazy"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></article></div><footer class=footer><p>&copy; 2024 <a href=https://proseguys.github.io/>人间客</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211 22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257m1.0199 2.67608C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257c-3.5504-6.28886-12.88753-4.410077-16.44303.0C2.88063 6.77451-.0433281 11.1668 1.38159 16.6571c.89322 3.4417 3.7911 5.6365 6.81584 6.1154M20.7188 5.04257c1.3509 1.89783 3.3111 6.34223 1.6353 10.37273M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814 11.1703 6.98257 11.0247 6.98456 10.9937 7.05061 10.5221 8.05496 9.07362 9.92941 8 10.945m3.0333-3.50056C10.9392 9.86549 11 15 12 17" stroke="currentcolor" stroke-linecap="round"/></svg>
</a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>